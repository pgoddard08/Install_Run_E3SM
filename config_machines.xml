<?xml version="1.0"?>

<!-- PG Created from Ben's, last edit 2/5/24  -->

<!-- This is an ordered list, not all fields are required, optional fields are noted below. -->
<config_machines version="2.0">
<!-- MACH is the name that you will use in machine options -->
  <machine MACH="BigRed200">

    <!-- DESC: a text description of the machine, this field is current not used in code-->
    <DESC>Big Red 200 is an HPE Cray EX supercomputer designed to support scientific and medical research, and advanced research in artificial intelligence, machine learning, and data analytics. Big Red 200 features 640 compute nodes, each equipped with 256 GB of memory and two 64-core, 2.25 GHz, 225-watt AMD EPYC 7742 processors. Big Red 200 also includes 64 GPU-accelerated nodes, each with 256 GB of memory, a single 64-core, 2.0 GHz, 225-watt AMD EPYC 7713 processor, and four NVIDIA A100 GPUs. Big Red 200 has a theoretical peak performance (Rpeak) nearly 7 petaFLOPS.</DESC>

    <!-- NODENAME_REGEX: a regular expression used to identify this machine
	 it must work on compute nodes as well as login nodes, use machine option
	 to create_test or create_newcase if this flag is not available -->
    <NODENAME_REGEX>.*.uits.iu.edu</NODENAME_REGEX>

    <!-- OS: the operating system of this machine. Passed to cppflags for
	 compiled programs as -DVALUE  recognized are LINUX, AIX, Darwin, CNL -->
    <OS>LINUX</OS>

    <!-- PROXY: optional http proxy for access to the internet-->
    <!--<PROXY> https://howto.get.out </PROXY>-->
    
    <!-- COMPILERS: compilers supported on this machine, comma separated list, first is default -->
    <!-- see config_machines_20240123.xml for setting up a gnu option as well -->
    <COMPILERS>intel,gnu</COMPILERS>

	<!-- MPILIBS: mpilibs supported on this machine, comma seperated list,
	     first is default, mpi-serial is assumed and not required in this list-->
    <MPILIBS>mpt</MPILIBS>

    <!-- PROJECT: A project or account number used for batch jobs
         This value is used for directory names. If different from
         actual accounting project id, use CHARGE_ACCOUNT
	 can be overridden in environment or $HOME/.cime/config -->
    <PROJECT>r00025</PROJECT>

    <!-- CHARGE_ACCOUNT: A project or account number used for batch jobs
	 This is the actual project used for cost accounting set in
         the batch script (ex. #PBS -A charge_account). Will default
         to PROJECT if not set.
	 can be overridden in environment or $HOME/.cime/config -->
    <CHARGE_ACCOUNT>r00025</CHARGE_ACCOUNT>

    <!-- SAVE_TIMING_DIR: (Acme only) directory for archiving timing output -->
    <SAVE_TIMING_DIR> </SAVE_TIMING_DIR>

    <!-- SAVE_TIMING_DIR_PROJECTS: (Acme only) projects whose jobs archive timing output -->
    <SAVE_TIMING_DIR_PROJECTS> </SAVE_TIMING_DIR_PROJECTS>

    <!-- CIME_OUTPUT_ROOT: Base directory for case output,
	 the case/bld and case/run directories are written below here -->
    <CIME_OUTPUT_ROOT>/N/scratch/$USER/cime_scratch</CIME_OUTPUT_ROOT>

    <!-- DIN_LOC_ROOT: location of the inputdata data directory
	 inputdata is downloaded automatically on a case by case basis as
	 long as the user has write access to this directory.   We recommend that
	 all cime model users on a system share an inputdata directory
	 as it can be quite large -->
    <DIN_LOC_ROOT>/N/project/obrienta_startup/datasets/CESMinputfiles</DIN_LOC_ROOT>

    <!-- DIN_LOC_ROOT_CLMFORC: override of DIN_LOC_ROOT specific to CLM
	 forcing data -->
    <DIN_LOC_ROOT_CLMFORC>/N/project/obrienta_startup/datasets/CESMinputfiles/atm/cam</DIN_LOC_ROOT_CLMFORC>

    <!-- DOUT_S_ROOT: root directory of short term archive files, short term
      archiving moves model output data out of the run directory, but
      keeps it on disk-->
    <DOUT_S_ROOT>$CIME_OUTPUT_ROOT/archive/$CASE</DOUT_S_ROOT>

    <!-- BASELINE_ROOT:  Root directory for system test baseline files -->
<!-- I don't have access to Travis' placement, so made my own in hopes that the program will download the files needed to this dir -->
    <BASELINE_ROOT>/N/project/MSB_Project/CESMv2.2.2/CESMinputfiles/cesm_baselines</BASELINE_ROOT>

    <!-- CCSM_CPRNC: for CESM, location of the cprnc tool, compares model output in testing-->
    <CCSM_CPRNC>/N/project/MSB_Project/CESMv2.2.2/my_cesm_sandbox/cime/tools/cprnc</CCSM_CPRNC>
    <!-- CCSM_CPRNC: for E3SM-->
    <!-- <CCSM_CPRNC> </CCSM_CPRNC>-->    

    <!-- GMAKE: gnu compatible make tool, default is 'gmake' -->
    <GMAKE>gmake</GMAKE>

    <!-- GMAKE_J: optional number of threads to pass to the gmake flag -->
    <GMAKE_J>8</GMAKE_J>

    <!-- BATCH_SYSTEM: batch system used on this machine,
      supported values are: none, cobalt, lsf, pbs, slurm -->
    <BATCH_SYSTEM>slurm</BATCH_SYSTEM>

    <!-- SUPPORTED_BY: contact information for support for this system
      this field is not used in code -->
    <SUPPORTED_BY>Paul B. Goddard or UITS Research Technologies</SUPPORTED_BY>

    <!-- MAX_TASKS_PER_NODE: maximum number of threads*tasks per
	 shared memory node on this machine,
	 should always be >= MAX_MPITASKS_PER_NODE -->
    <MAX_TASKS_PER_NODE>128</MAX_TASKS_PER_NODE>

    <!--Maximum number of GPUs per node -->
    <!--MAX_GPUS_PER_NODE>4</MAX_GPUS_PER_NODE -->

    <!-- MAX_MPITASKS_PER_NODE: number of physical PES per shared node on
	 this machine, in practice the MPI tasks per node will not exceed this value -->
    <MAX_MPITASKS_PER_NODE>128</MAX_MPITASKS_PER_NODE>

    <!-- PROJECT_REQUIRED: Does this machine require a project to be specified to
	 the batch system?  See PROJECT above -->
    <PROJECT_REQUIRED>TRUE</PROJECT_REQUIRED>

    <!-- mpirun: The mpi exec to start a job on this machine, supported values
	 are values listed in MPILIBS above, default and mpi-serial -->
    <mpirun mpilib="default">
      <!-- name of the exectuable used to launch mpi jobs -->
      <executable>srun</executable>
      <!-- arguments to the mpiexec command, the name attribute here is ignored-->
      <arguments>
                <arg name="label"> --label</arg>
                <arg name="num_tasks"> -n {{ total_tasks }}</arg>
                <!--<arg name="binding"> -c {{ srun_binding }}</arg> -->   
                <!--<arg name="tasks_per_numa"> -S {{ tasks_per_numa }}</arg>-->
                <!--<arg name="tasks_per_node"> -N $MAX_TASKS_PER_NODE</arg>-->
                <!--<arg name="thread_count"> -d $ENV{OMP_NUM_THREADS}</arg>-->
	<!--<arg name="labelstdout">-p "%g:"</arg>
	<arg name="threadplacement"> omplace </arg>-->
      </arguments>
    </mpirun>
    <mpirun mpilib="mpi-serial">
      <executable>srun</executable>
    </mpirun>
    <!-- module system: allowed module_system type values are:
	 module  http://www.tacc.utexas.edu/tacc-projects/mclay/lmod
	 soft http://www.mcs.anl.gov/hs/software/systems/softenv/softenv-intro.html
         none
      -->
    <module_system type="module">
      <init_path lang="perl">/usr/share/lmod/lmod/init/perl.pm</init_path>
      <init_path lang="sh">/usr/share/lmod/lmod/init/sh</init_path>
      <init_path lang="csh">/usr/share/lmod/lmod/init/csh</init_path>
      <init_path lang="python">/usr/share/lmod/lmod/init/python.py</init_path>
      <cmd_path lang="perl">/usr/share/lmod/lmod/libexec/lmod perl</cmd_path>
      <cmd_path lang="python">/usr/share/lmod/lmod/libexec/lmod python -q</cmd_path>
      <cmd_path lang="sh">module</cmd_path>
      <cmd_path lang="csh">module</cmd_path>
<!--
      <modules>
        <command name="rm">PrgEnv-intel</command>
        <command name="rm">PrgEnv-cray</command>
        <command name="rm">PrgEnv-gnu</command>
        <command name="rm">intel</command>
        <command name="rm">cce</command>
        <command name="rm">cray-parallel-netcdf</command>
        <command name="rm">cray-parallel-hdf5</command>
        <command name="rm">pmi</command>
        <command name="rm">cray-libsci</command>
        <command name="rm">cray-mpich2</command>
        <command name="rm">cray-mpich</command>
        <command name="rm">cray-netcdf</command>
        <command name="rm">cray-hdf5</command>
        <command name="rm">cray-netcdf-hdf5parallel</command>
        <command name="rm">craype-sandybridge</command>
        <command name="rm">craype-ivybridge</command>
        <command name="rm">craype</command>
        <command name="rm">gcc</command>
      </modules>
-->
      <modules compiler="intel">
        <command name="load">PrgEnv-intel</command>
      </modules>
      <modules compiler="gnu">
        <command name="load">PrgEnv-gnu</command>
      </modules>

      <modules>
        <command name="load">cray-mpich/8.1.16</command>
      </modules>
      <modules>
        <command name="load">cray-libsci</command>
      </modules>
      <modules>
        <command name="load">cmake</command>
      </modules>
      <modules mpilib="!mpi-serial">
        <command name="load">intel/2021.2.0</command>
        <command name="load">cray-hdf5-parallel/1.12.1.3</command>
        <command name="load">cray-netcdf-hdf5parallel/4.8.1.3</command>
        <command name="load">cray-parallel-netcdf</command>
        <!--<command name="load">cray-mpich/8.1.16</command>-->
        <!--<command name="load">cray-libsci</command>-->
        <!--<command name="load">cmake</command>-->
      </modules>
    </module_system>
<!-- environment variables, a blank entry will unset a variable -->
    <environment_variables>
      <env name="OMP_STACKSIZE">256M</env>
      <env name="OMP_PROC_BIND">spread</env>
      <env name="OMP_PLACES">threads</env>
      <env name="MPI_TYPE_DEPTH">16</env>
      <!-- <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env> -->
      <env name="MKLROOT">$ENV{MKLROOT} </env>
      <env name="MPICH_RANK_REORDER_METHOD">0</env>
    </environment_variables>
    <environment_variables compiler="intel">
      <!-- <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env> -->
      <env name="NETCDF_PATH">/opt/cray/pe/netcdf-hdf5parallel/4.8.1.3/intel/19.0</env>
    </environment_variables>
    <environment_variables compiler="gnu">
      <!-- <env name="NETCDF_PATH">$ENV{NETCDF_DIR}</env> -->
      <env name="NETCDF_PATH">/opt/cray/pe/netcdf-hdf5parallel/4.8.1.3/gnu/9.1</env>
    </environment_variables>
    <!-- resource settings as defined in https://docs.python.org/2/library/resource.html -->
    <!--<resource_limits>
      <resource name="RLIMIT_STACK">-1</resource>
    </resource_limits>-->

  </machine>
</config_machines>
